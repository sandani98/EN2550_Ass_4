{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('cv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "acb22a8b69de88320319ef34cebc68b783b44c7a14d82e341757e6edeba099a7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Part III - Two layer FC network Sigmoid function included with Stochastic Gradient Descend"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iteration 25000 / 30000 : loss 0.900013\n",
      "iteration 25010 / 30000 : loss 0.900022\n",
      "iteration 25020 / 30000 : loss 0.900021\n",
      "iteration 25030 / 30000 : loss 0.899971\n",
      "iteration 25040 / 30000 : loss 0.899962\n",
      "iteration 25050 / 30000 : loss 0.899972\n",
      "iteration 25060 / 30000 : loss 0.899968\n",
      "iteration 25070 / 30000 : loss 0.900005\n",
      "iteration 25080 / 30000 : loss 0.900012\n",
      "iteration 25090 / 30000 : loss 0.900042\n",
      "iteration 25100 / 30000 : loss 0.900029\n",
      "iteration 25110 / 30000 : loss 0.900040\n",
      "iteration 25120 / 30000 : loss 0.899977\n",
      "iteration 25130 / 30000 : loss 0.900004\n",
      "iteration 25140 / 30000 : loss 0.899995\n",
      "iteration 25150 / 30000 : loss 0.899983\n",
      "iteration 25160 / 30000 : loss 0.899993\n",
      "iteration 25170 / 30000 : loss 0.900003\n",
      "iteration 25180 / 30000 : loss 0.899995\n",
      "iteration 25190 / 30000 : loss 0.899982\n",
      "iteration 25200 / 30000 : loss 0.900026\n",
      "iteration 25210 / 30000 : loss 0.899992\n",
      "iteration 25220 / 30000 : loss 0.899992\n",
      "iteration 25230 / 30000 : loss 0.900033\n",
      "iteration 25240 / 30000 : loss 0.899988\n",
      "iteration 25250 / 30000 : loss 0.899987\n",
      "iteration 25260 / 30000 : loss 0.899989\n",
      "iteration 25270 / 30000 : loss 0.900009\n",
      "iteration 25280 / 30000 : loss 0.899959\n",
      "iteration 25290 / 30000 : loss 0.899978\n",
      "iteration 25300 / 30000 : loss 0.900019\n",
      "iteration 25310 / 30000 : loss 0.899936\n",
      "iteration 25320 / 30000 : loss 0.899969\n",
      "iteration 25330 / 30000 : loss 0.899950\n",
      "iteration 25340 / 30000 : loss 0.899988\n",
      "iteration 25350 / 30000 : loss 0.900002\n",
      "iteration 25360 / 30000 : loss 0.900005\n",
      "iteration 25370 / 30000 : loss 0.899995\n",
      "iteration 25380 / 30000 : loss 0.900042\n",
      "iteration 25390 / 30000 : loss 0.899979\n",
      "iteration 25400 / 30000 : loss 0.899983\n",
      "iteration 25410 / 30000 : loss 0.900000\n",
      "iteration 25420 / 30000 : loss 0.900009\n",
      "iteration 25430 / 30000 : loss 0.899995\n",
      "iteration 25440 / 30000 : loss 0.899962\n",
      "iteration 25450 / 30000 : loss 0.899995\n",
      "iteration 25460 / 30000 : loss 0.899999\n",
      "iteration 25470 / 30000 : loss 0.900010\n",
      "iteration 25480 / 30000 : loss 0.899989\n",
      "iteration 25490 / 30000 : loss 0.900001\n",
      "iteration 25500 / 30000 : loss 0.900001\n",
      "iteration 25510 / 30000 : loss 0.899980\n",
      "iteration 25520 / 30000 : loss 0.899973\n",
      "iteration 25530 / 30000 : loss 0.900064\n",
      "iteration 25540 / 30000 : loss 0.900010\n",
      "iteration 25550 / 30000 : loss 0.900021\n",
      "iteration 25560 / 30000 : loss 0.900021\n",
      "iteration 25570 / 30000 : loss 0.900021\n",
      "iteration 25580 / 30000 : loss 0.899998\n",
      "iteration 25590 / 30000 : loss 0.900009\n",
      "iteration 25600 / 30000 : loss 0.899993\n",
      "iteration 25610 / 30000 : loss 0.900008\n",
      "iteration 25620 / 30000 : loss 0.900003\n",
      "iteration 25630 / 30000 : loss 0.899968\n",
      "iteration 25640 / 30000 : loss 0.899986\n",
      "iteration 25650 / 30000 : loss 0.900004\n",
      "iteration 25660 / 30000 : loss 0.899980\n",
      "iteration 25670 / 30000 : loss 0.900059\n",
      "iteration 25680 / 30000 : loss 0.899974\n",
      "iteration 25690 / 30000 : loss 0.899984\n",
      "iteration 25700 / 30000 : loss 0.900015\n",
      "iteration 25710 / 30000 : loss 0.899994\n",
      "iteration 25720 / 30000 : loss 0.900020\n",
      "iteration 25730 / 30000 : loss 0.899978\n",
      "iteration 25740 / 30000 : loss 0.900023\n",
      "iteration 25750 / 30000 : loss 0.899981\n",
      "iteration 25760 / 30000 : loss 0.899958\n",
      "iteration 25770 / 30000 : loss 0.899991\n",
      "iteration 25780 / 30000 : loss 0.899986\n",
      "iteration 25790 / 30000 : loss 0.899998\n",
      "iteration 25800 / 30000 : loss 0.899988\n",
      "iteration 25810 / 30000 : loss 0.900006\n",
      "iteration 25820 / 30000 : loss 0.900013\n",
      "iteration 25830 / 30000 : loss 0.899991\n",
      "iteration 25840 / 30000 : loss 0.899996\n",
      "iteration 25850 / 30000 : loss 0.899977\n",
      "iteration 25860 / 30000 : loss 0.900014\n",
      "iteration 25870 / 30000 : loss 0.899997\n",
      "iteration 25880 / 30000 : loss 0.899986\n",
      "iteration 25890 / 30000 : loss 0.900008\n",
      "iteration 25900 / 30000 : loss 0.900013\n",
      "iteration 25910 / 30000 : loss 0.899994\n",
      "iteration 25920 / 30000 : loss 0.900002\n",
      "iteration 25930 / 30000 : loss 0.900005\n",
      "iteration 25940 / 30000 : loss 0.899996\n",
      "iteration 25950 / 30000 : loss 0.899962\n",
      "iteration 25960 / 30000 : loss 0.900028\n",
      "iteration 25970 / 30000 : loss 0.900003\n",
      "iteration 25980 / 30000 : loss 0.900011\n",
      "iteration 25990 / 30000 : loss 0.900032\n",
      "iteration 26000 / 30000 : loss 0.900004\n",
      "iteration 26010 / 30000 : loss 0.899959\n",
      "iteration 26020 / 30000 : loss 0.900014\n",
      "iteration 26030 / 30000 : loss 0.900004\n",
      "iteration 26040 / 30000 : loss 0.900021\n",
      "iteration 26050 / 30000 : loss 0.900000\n",
      "iteration 26060 / 30000 : loss 0.899982\n",
      "iteration 26070 / 30000 : loss 0.900009\n",
      "iteration 26080 / 30000 : loss 0.900049\n",
      "iteration 26090 / 30000 : loss 0.899983\n",
      "iteration 26100 / 30000 : loss 0.900028\n",
      "iteration 26110 / 30000 : loss 0.900007\n",
      "iteration 26120 / 30000 : loss 0.899986\n",
      "iteration 26130 / 30000 : loss 0.899973\n",
      "iteration 26140 / 30000 : loss 0.899987\n",
      "iteration 26150 / 30000 : loss 0.899957\n",
      "iteration 26160 / 30000 : loss 0.900006\n",
      "iteration 26170 / 30000 : loss 0.900000\n",
      "iteration 26180 / 30000 : loss 0.900060\n",
      "iteration 26190 / 30000 : loss 0.899978\n",
      "iteration 26200 / 30000 : loss 0.899990\n",
      "iteration 26210 / 30000 : loss 0.899996\n",
      "iteration 26220 / 30000 : loss 0.899977\n",
      "iteration 26230 / 30000 : loss 0.900014\n",
      "iteration 26240 / 30000 : loss 0.899995\n",
      "iteration 26250 / 30000 : loss 0.900007\n",
      "iteration 26260 / 30000 : loss 0.899961\n",
      "iteration 26270 / 30000 : loss 0.899984\n",
      "iteration 26280 / 30000 : loss 0.899992\n",
      "iteration 26290 / 30000 : loss 0.900002\n",
      "iteration 26300 / 30000 : loss 0.899982\n",
      "iteration 26310 / 30000 : loss 0.900009\n",
      "iteration 26320 / 30000 : loss 0.899999\n",
      "iteration 26330 / 30000 : loss 0.900027\n",
      "iteration 26340 / 30000 : loss 0.899940\n",
      "iteration 26350 / 30000 : loss 0.899986\n",
      "iteration 26360 / 30000 : loss 0.900005\n",
      "iteration 26370 / 30000 : loss 0.900023\n",
      "iteration 26380 / 30000 : loss 0.900007\n",
      "iteration 26390 / 30000 : loss 0.900018\n",
      "iteration 26400 / 30000 : loss 0.900004\n",
      "iteration 26410 / 30000 : loss 0.900021\n",
      "iteration 26420 / 30000 : loss 0.899970\n",
      "iteration 26430 / 30000 : loss 0.899999\n",
      "iteration 26440 / 30000 : loss 0.900025\n",
      "iteration 26450 / 30000 : loss 0.900002\n",
      "iteration 26460 / 30000 : loss 0.899976\n",
      "iteration 26470 / 30000 : loss 0.899962\n",
      "iteration 26480 / 30000 : loss 0.900040\n",
      "iteration 26490 / 30000 : loss 0.900021\n",
      "iteration 26500 / 30000 : loss 0.900000\n",
      "iteration 26510 / 30000 : loss 0.900006\n",
      "iteration 26520 / 30000 : loss 0.899976\n",
      "iteration 26530 / 30000 : loss 0.899980\n",
      "iteration 26540 / 30000 : loss 0.900029\n",
      "iteration 26550 / 30000 : loss 0.899995\n",
      "iteration 26560 / 30000 : loss 0.900024\n",
      "iteration 26570 / 30000 : loss 0.900059\n",
      "iteration 26580 / 30000 : loss 0.899997\n",
      "iteration 26590 / 30000 : loss 0.899978\n",
      "iteration 26600 / 30000 : loss 0.900014\n",
      "iteration 26610 / 30000 : loss 0.900035\n",
      "iteration 26620 / 30000 : loss 0.899971\n",
      "iteration 26630 / 30000 : loss 0.900043\n",
      "iteration 26640 / 30000 : loss 0.900043\n",
      "iteration 26650 / 30000 : loss 0.900006\n",
      "iteration 26660 / 30000 : loss 0.899987\n",
      "iteration 26670 / 30000 : loss 0.900014\n",
      "iteration 26680 / 30000 : loss 0.899982\n",
      "iteration 26690 / 30000 : loss 0.900024\n",
      "iteration 26700 / 30000 : loss 0.900013\n",
      "iteration 26710 / 30000 : loss 0.899999\n",
      "iteration 26720 / 30000 : loss 0.900001\n",
      "iteration 26730 / 30000 : loss 0.900012\n",
      "iteration 26740 / 30000 : loss 0.900032\n",
      "iteration 26750 / 30000 : loss 0.900020\n",
      "iteration 26760 / 30000 : loss 0.900015\n",
      "iteration 26770 / 30000 : loss 0.899960\n",
      "iteration 26780 / 30000 : loss 0.900044\n",
      "iteration 26790 / 30000 : loss 0.900002\n",
      "iteration 26800 / 30000 : loss 0.899981\n",
      "iteration 26810 / 30000 : loss 0.900018\n",
      "iteration 26820 / 30000 : loss 0.899968\n",
      "iteration 26830 / 30000 : loss 0.899992\n",
      "iteration 26840 / 30000 : loss 0.899971\n",
      "iteration 26850 / 30000 : loss 0.900013\n",
      "iteration 26860 / 30000 : loss 0.900007\n",
      "iteration 26870 / 30000 : loss 0.899977\n",
      "iteration 26880 / 30000 : loss 0.900032\n",
      "iteration 26890 / 30000 : loss 0.900024\n",
      "iteration 26900 / 30000 : loss 0.900002\n",
      "iteration 26910 / 30000 : loss 0.900010\n",
      "iteration 26920 / 30000 : loss 0.900047\n",
      "iteration 26930 / 30000 : loss 0.899972\n",
      "iteration 26940 / 30000 : loss 0.900053\n",
      "iteration 26950 / 30000 : loss 0.899983\n",
      "iteration 26960 / 30000 : loss 0.900045\n",
      "iteration 26970 / 30000 : loss 0.899998\n",
      "iteration 26980 / 30000 : loss 0.900030\n",
      "iteration 26990 / 30000 : loss 0.900019\n",
      "iteration 27000 / 30000 : loss 0.899997\n",
      "iteration 27010 / 30000 : loss 0.899982\n",
      "iteration 27020 / 30000 : loss 0.900014\n",
      "iteration 27030 / 30000 : loss 0.899971\n",
      "iteration 27040 / 30000 : loss 0.900015\n",
      "iteration 27050 / 30000 : loss 0.900025\n",
      "iteration 27060 / 30000 : loss 0.900018\n",
      "iteration 27070 / 30000 : loss 0.899978\n",
      "iteration 27080 / 30000 : loss 0.900012\n",
      "iteration 27090 / 30000 : loss 0.899973\n",
      "iteration 27100 / 30000 : loss 0.900007\n",
      "iteration 27110 / 30000 : loss 0.900001\n",
      "iteration 27120 / 30000 : loss 0.899980\n",
      "iteration 27130 / 30000 : loss 0.900007\n",
      "iteration 27140 / 30000 : loss 0.899956\n",
      "iteration 27150 / 30000 : loss 0.899987\n",
      "iteration 27160 / 30000 : loss 0.900013\n",
      "iteration 27170 / 30000 : loss 0.899971\n",
      "iteration 27180 / 30000 : loss 0.900006\n",
      "iteration 27190 / 30000 : loss 0.899997\n",
      "iteration 27200 / 30000 : loss 0.900011\n",
      "iteration 27210 / 30000 : loss 0.899985\n",
      "iteration 27220 / 30000 : loss 0.900008\n",
      "iteration 27230 / 30000 : loss 0.900008\n",
      "iteration 27240 / 30000 : loss 0.900057\n",
      "iteration 27250 / 30000 : loss 0.899987\n",
      "iteration 27260 / 30000 : loss 0.900037\n",
      "iteration 27270 / 30000 : loss 0.900022\n",
      "iteration 27280 / 30000 : loss 0.899986\n",
      "iteration 27290 / 30000 : loss 0.900006\n",
      "iteration 27300 / 30000 : loss 0.899965\n",
      "iteration 27310 / 30000 : loss 0.900012\n",
      "iteration 27320 / 30000 : loss 0.899989\n",
      "iteration 27330 / 30000 : loss 0.899986\n",
      "iteration 27340 / 30000 : loss 0.900007\n",
      "iteration 27350 / 30000 : loss 0.899990\n",
      "iteration 27360 / 30000 : loss 0.899996\n",
      "iteration 27370 / 30000 : loss 0.900016\n",
      "iteration 27380 / 30000 : loss 0.899991\n",
      "iteration 27390 / 30000 : loss 0.900014\n",
      "iteration 27400 / 30000 : loss 0.900042\n",
      "iteration 27410 / 30000 : loss 0.899963\n",
      "iteration 27420 / 30000 : loss 0.899981\n",
      "iteration 27430 / 30000 : loss 0.900003\n",
      "iteration 27440 / 30000 : loss 0.900005\n",
      "iteration 27450 / 30000 : loss 0.900015\n",
      "iteration 27460 / 30000 : loss 0.900016\n",
      "iteration 27470 / 30000 : loss 0.900031\n",
      "iteration 27480 / 30000 : loss 0.900007\n",
      "iteration 27490 / 30000 : loss 0.899986\n",
      "iteration 27500 / 30000 : loss 0.900004\n",
      "iteration 27510 / 30000 : loss 0.900015\n",
      "iteration 27520 / 30000 : loss 0.900005\n",
      "iteration 27530 / 30000 : loss 0.900025\n",
      "iteration 27540 / 30000 : loss 0.899943\n",
      "iteration 27550 / 30000 : loss 0.900005\n",
      "iteration 27560 / 30000 : loss 0.900000\n",
      "iteration 27570 / 30000 : loss 0.900010\n",
      "iteration 27580 / 30000 : loss 0.900021\n",
      "iteration 27590 / 30000 : loss 0.899994\n",
      "iteration 27600 / 30000 : loss 0.900019\n",
      "iteration 27610 / 30000 : loss 0.899969\n",
      "iteration 27620 / 30000 : loss 0.900025\n",
      "iteration 27630 / 30000 : loss 0.899995\n",
      "iteration 27640 / 30000 : loss 0.900018\n",
      "iteration 27650 / 30000 : loss 0.899994\n",
      "iteration 27660 / 30000 : loss 0.899968\n",
      "iteration 27670 / 30000 : loss 0.899994\n",
      "iteration 27680 / 30000 : loss 0.900030\n",
      "iteration 27690 / 30000 : loss 0.899987\n",
      "iteration 27700 / 30000 : loss 0.899981\n",
      "iteration 27710 / 30000 : loss 0.900017\n",
      "iteration 27720 / 30000 : loss 0.900049\n",
      "iteration 27730 / 30000 : loss 0.899998\n",
      "iteration 27740 / 30000 : loss 0.900012\n",
      "iteration 27750 / 30000 : loss 0.899986\n",
      "iteration 27760 / 30000 : loss 0.900007\n",
      "iteration 27770 / 30000 : loss 0.899999\n",
      "iteration 27780 / 30000 : loss 0.900023\n",
      "iteration 27790 / 30000 : loss 0.900034\n",
      "iteration 27800 / 30000 : loss 0.900003\n",
      "iteration 27810 / 30000 : loss 0.899974\n",
      "iteration 27820 / 30000 : loss 0.900053\n",
      "iteration 27830 / 30000 : loss 0.900024\n",
      "iteration 27840 / 30000 : loss 0.900007\n",
      "iteration 27850 / 30000 : loss 0.900009\n",
      "iteration 27860 / 30000 : loss 0.900015\n",
      "iteration 27870 / 30000 : loss 0.899968\n",
      "iteration 27880 / 30000 : loss 0.899993\n",
      "iteration 27890 / 30000 : loss 0.900003\n",
      "iteration 27900 / 30000 : loss 0.899990\n",
      "iteration 27910 / 30000 : loss 0.900015\n",
      "iteration 27920 / 30000 : loss 0.899985\n",
      "iteration 27930 / 30000 : loss 0.899975\n",
      "iteration 27940 / 30000 : loss 0.900014\n",
      "iteration 27950 / 30000 : loss 0.900030\n",
      "iteration 27960 / 30000 : loss 0.899997\n",
      "iteration 27970 / 30000 : loss 0.900009\n",
      "iteration 27980 / 30000 : loss 0.900017\n",
      "iteration 27990 / 30000 : loss 0.899990\n",
      "iteration 28000 / 30000 : loss 0.900014\n",
      "iteration 28010 / 30000 : loss 0.900034\n",
      "iteration 28020 / 30000 : loss 0.900014\n",
      "iteration 28030 / 30000 : loss 0.899988\n",
      "iteration 28040 / 30000 : loss 0.900008\n",
      "iteration 28050 / 30000 : loss 0.900017\n",
      "iteration 28060 / 30000 : loss 0.900012\n",
      "iteration 28070 / 30000 : loss 0.900016\n",
      "iteration 28080 / 30000 : loss 0.899971\n",
      "iteration 28090 / 30000 : loss 0.900011\n",
      "iteration 28100 / 30000 : loss 0.899988\n",
      "iteration 28110 / 30000 : loss 0.900005\n",
      "iteration 28120 / 30000 : loss 0.900012\n",
      "iteration 28130 / 30000 : loss 0.900010\n",
      "iteration 28140 / 30000 : loss 0.899976\n",
      "iteration 28150 / 30000 : loss 0.899978\n",
      "iteration 28160 / 30000 : loss 0.899993\n",
      "iteration 28170 / 30000 : loss 0.900007\n",
      "iteration 28180 / 30000 : loss 0.899988\n",
      "iteration 28190 / 30000 : loss 0.899995\n",
      "iteration 28200 / 30000 : loss 0.900009\n",
      "iteration 28210 / 30000 : loss 0.899982\n",
      "iteration 28220 / 30000 : loss 0.900053\n",
      "iteration 28230 / 30000 : loss 0.899994\n",
      "iteration 28240 / 30000 : loss 0.900044\n",
      "iteration 28250 / 30000 : loss 0.900022\n",
      "iteration 28260 / 30000 : loss 0.900038\n",
      "iteration 28270 / 30000 : loss 0.900031\n",
      "iteration 28280 / 30000 : loss 0.899994\n",
      "iteration 28290 / 30000 : loss 0.899929\n",
      "iteration 28300 / 30000 : loss 0.899992\n",
      "iteration 28310 / 30000 : loss 0.900014\n",
      "iteration 28320 / 30000 : loss 0.899993\n",
      "iteration 28330 / 30000 : loss 0.899960\n",
      "iteration 28340 / 30000 : loss 0.899983\n",
      "iteration 28350 / 30000 : loss 0.900003\n",
      "iteration 28360 / 30000 : loss 0.900022\n",
      "iteration 28370 / 30000 : loss 0.900002\n",
      "iteration 28380 / 30000 : loss 0.900021\n",
      "iteration 28390 / 30000 : loss 0.899961\n",
      "iteration 28400 / 30000 : loss 0.900017\n",
      "iteration 28410 / 30000 : loss 0.900026\n",
      "iteration 28420 / 30000 : loss 0.899994\n",
      "iteration 28430 / 30000 : loss 0.899982\n",
      "iteration 28440 / 30000 : loss 0.899998\n",
      "iteration 28450 / 30000 : loss 0.899988\n",
      "iteration 28460 / 30000 : loss 0.900022\n",
      "iteration 28470 / 30000 : loss 0.900013\n",
      "iteration 28480 / 30000 : loss 0.900011\n",
      "iteration 28490 / 30000 : loss 0.900001\n",
      "iteration 28500 / 30000 : loss 0.900018\n",
      "iteration 28510 / 30000 : loss 0.899997\n",
      "iteration 28520 / 30000 : loss 0.899961\n",
      "iteration 28530 / 30000 : loss 0.900017\n",
      "iteration 28540 / 30000 : loss 0.900003\n",
      "iteration 28550 / 30000 : loss 0.900002\n",
      "iteration 28560 / 30000 : loss 0.900019\n",
      "iteration 28570 / 30000 : loss 0.899999\n",
      "iteration 28580 / 30000 : loss 0.900022\n",
      "iteration 28590 / 30000 : loss 0.900000\n",
      "iteration 28600 / 30000 : loss 0.900015\n",
      "iteration 28610 / 30000 : loss 0.900032\n",
      "iteration 28620 / 30000 : loss 0.900035\n",
      "iteration 28630 / 30000 : loss 0.899994\n",
      "iteration 28640 / 30000 : loss 0.900051\n",
      "iteration 28650 / 30000 : loss 0.900003\n",
      "iteration 28660 / 30000 : loss 0.899987\n",
      "iteration 28670 / 30000 : loss 0.900010\n",
      "iteration 28680 / 30000 : loss 0.899975\n",
      "iteration 28690 / 30000 : loss 0.900016\n",
      "iteration 28700 / 30000 : loss 0.899952\n",
      "iteration 28710 / 30000 : loss 0.899992\n",
      "iteration 28720 / 30000 : loss 0.899995\n",
      "iteration 28730 / 30000 : loss 0.900001\n",
      "iteration 28740 / 30000 : loss 0.900025\n",
      "iteration 28750 / 30000 : loss 0.899965\n",
      "iteration 28760 / 30000 : loss 0.899995\n",
      "iteration 28770 / 30000 : loss 0.900017\n",
      "iteration 28780 / 30000 : loss 0.900020\n",
      "iteration 28790 / 30000 : loss 0.900006\n",
      "iteration 28800 / 30000 : loss 0.899994\n",
      "iteration 28810 / 30000 : loss 0.899970\n",
      "iteration 28820 / 30000 : loss 0.899984\n",
      "iteration 28830 / 30000 : loss 0.900031\n",
      "iteration 28840 / 30000 : loss 0.900045\n",
      "iteration 28850 / 30000 : loss 0.900045\n",
      "iteration 28860 / 30000 : loss 0.900006\n",
      "iteration 28870 / 30000 : loss 0.899989\n",
      "iteration 28880 / 30000 : loss 0.900024\n",
      "iteration 28890 / 30000 : loss 0.900009\n",
      "iteration 28900 / 30000 : loss 0.900000\n",
      "iteration 28910 / 30000 : loss 0.900036\n",
      "iteration 28920 / 30000 : loss 0.899979\n",
      "iteration 28930 / 30000 : loss 0.899967\n",
      "iteration 28940 / 30000 : loss 0.899976\n",
      "iteration 28950 / 30000 : loss 0.900004\n",
      "iteration 28960 / 30000 : loss 0.900042\n",
      "iteration 28970 / 30000 : loss 0.899967\n",
      "iteration 28980 / 30000 : loss 0.900031\n",
      "iteration 28990 / 30000 : loss 0.900032\n",
      "iteration 29000 / 30000 : loss 0.899985\n",
      "iteration 29010 / 30000 : loss 0.899995\n",
      "iteration 29020 / 30000 : loss 0.900010\n",
      "iteration 29030 / 30000 : loss 0.899989\n",
      "iteration 29040 / 30000 : loss 0.900000\n",
      "iteration 29050 / 30000 : loss 0.900031\n",
      "iteration 29060 / 30000 : loss 0.900011\n",
      "iteration 29070 / 30000 : loss 0.899985\n",
      "iteration 29080 / 30000 : loss 0.900012\n",
      "iteration 29090 / 30000 : loss 0.900029\n",
      "iteration 29100 / 30000 : loss 0.900001\n",
      "iteration 29110 / 30000 : loss 0.899979\n",
      "iteration 29120 / 30000 : loss 0.900000\n",
      "iteration 29130 / 30000 : loss 0.899992\n",
      "iteration 29140 / 30000 : loss 0.900001\n",
      "iteration 29150 / 30000 : loss 0.900021\n",
      "iteration 29160 / 30000 : loss 0.900007\n",
      "iteration 29170 / 30000 : loss 0.900003\n",
      "iteration 29180 / 30000 : loss 0.900026\n",
      "iteration 29190 / 30000 : loss 0.899996\n",
      "iteration 29200 / 30000 : loss 0.899990\n",
      "iteration 29210 / 30000 : loss 0.900006\n",
      "iteration 29220 / 30000 : loss 0.899972\n",
      "iteration 29230 / 30000 : loss 0.900038\n",
      "iteration 29240 / 30000 : loss 0.900009\n",
      "iteration 29250 / 30000 : loss 0.899960\n",
      "iteration 29260 / 30000 : loss 0.899991\n",
      "iteration 29270 / 30000 : loss 0.900011\n",
      "iteration 29280 / 30000 : loss 0.899986\n",
      "iteration 29290 / 30000 : loss 0.900000\n",
      "iteration 29300 / 30000 : loss 0.899970\n",
      "iteration 29310 / 30000 : loss 0.900057\n",
      "iteration 29320 / 30000 : loss 0.899989\n",
      "iteration 29330 / 30000 : loss 0.899992\n",
      "iteration 29340 / 30000 : loss 0.900032\n",
      "iteration 29350 / 30000 : loss 0.899998\n",
      "iteration 29360 / 30000 : loss 0.900020\n",
      "iteration 29370 / 30000 : loss 0.900000\n",
      "iteration 29380 / 30000 : loss 0.899968\n",
      "iteration 29390 / 30000 : loss 0.899974\n",
      "iteration 29400 / 30000 : loss 0.899990\n",
      "iteration 29410 / 30000 : loss 0.899990\n",
      "iteration 29420 / 30000 : loss 0.899986\n",
      "iteration 29430 / 30000 : loss 0.899962\n",
      "iteration 29440 / 30000 : loss 0.899991\n",
      "iteration 29450 / 30000 : loss 0.899994\n",
      "iteration 29460 / 30000 : loss 0.899992\n",
      "iteration 29470 / 30000 : loss 0.899967\n",
      "iteration 29480 / 30000 : loss 0.899977\n",
      "iteration 29490 / 30000 : loss 0.900001\n",
      "iteration 29500 / 30000 : loss 0.899958\n",
      "iteration 29510 / 30000 : loss 0.899982\n",
      "iteration 29520 / 30000 : loss 0.899980\n",
      "iteration 29530 / 30000 : loss 0.900027\n",
      "iteration 29540 / 30000 : loss 0.899984\n",
      "iteration 29550 / 30000 : loss 0.900004\n",
      "iteration 29560 / 30000 : loss 0.899970\n",
      "iteration 29570 / 30000 : loss 0.899990\n",
      "iteration 29580 / 30000 : loss 0.899996\n",
      "iteration 29590 / 30000 : loss 0.900015\n",
      "iteration 29600 / 30000 : loss 0.899996\n",
      "iteration 29610 / 30000 : loss 0.900010\n",
      "iteration 29620 / 30000 : loss 0.900044\n",
      "iteration 29630 / 30000 : loss 0.900007\n",
      "iteration 29640 / 30000 : loss 0.899990\n",
      "iteration 29650 / 30000 : loss 0.900014\n",
      "iteration 29660 / 30000 : loss 0.900006\n",
      "iteration 29670 / 30000 : loss 0.899987\n",
      "iteration 29680 / 30000 : loss 0.900020\n",
      "iteration 29690 / 30000 : loss 0.899959\n",
      "iteration 29700 / 30000 : loss 0.900027\n",
      "iteration 29710 / 30000 : loss 0.899966\n",
      "iteration 29720 / 30000 : loss 0.900006\n",
      "iteration 29730 / 30000 : loss 0.900006\n",
      "iteration 29740 / 30000 : loss 0.900024\n",
      "iteration 29750 / 30000 : loss 0.900014\n",
      "iteration 29760 / 30000 : loss 0.900006\n",
      "iteration 29770 / 30000 : loss 0.899970\n",
      "iteration 29780 / 30000 : loss 0.900012\n",
      "iteration 29790 / 30000 : loss 0.899982\n",
      "iteration 29800 / 30000 : loss 0.900023\n",
      "iteration 29810 / 30000 : loss 0.899972\n",
      "iteration 29820 / 30000 : loss 0.900042\n",
      "iteration 29830 / 30000 : loss 0.900032\n",
      "iteration 29840 / 30000 : loss 0.899985\n",
      "iteration 29850 / 30000 : loss 0.900001\n",
      "iteration 29860 / 30000 : loss 0.899995\n",
      "iteration 29870 / 30000 : loss 0.900011\n",
      "iteration 29880 / 30000 : loss 0.900011\n",
      "iteration 29890 / 30000 : loss 0.899966\n",
      "iteration 29900 / 30000 : loss 0.900006\n",
      "iteration 29910 / 30000 : loss 0.899983\n",
      "iteration 29920 / 30000 : loss 0.899973\n",
      "iteration 29930 / 30000 : loss 0.900000\n",
      "iteration 29940 / 30000 : loss 0.899999\n",
      "iteration 29950 / 30000 : loss 0.899990\n",
      "iteration 29960 / 30000 : loss 0.899979\n",
      "iteration 29970 / 30000 : loss 0.900005\n",
      "iteration 29980 / 30000 : loss 0.899995\n",
      "iteration 29990 / 30000 : loss 0.899999\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "print('x_train: ', x_train.shape)\n",
    "\n",
    "\n",
    "K = len(np.unique(y_train)) # Classes\n",
    "Ntr = x_train.shape[0]\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10\n",
    "# Din = 784 # MINIST\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "H = 200  #Hidden layers\n",
    "std=1e-5\n",
    "w1 = std*np.random.randn(Din, H)\n",
    "b1 = np.zeros(H)\n",
    "w2 = std*np.random.randn(H, K)\n",
    "b2 = np.zeros(K)\n",
    "print(\"w1:\", w1.shape)\n",
    "print(\"b1:\", b1.shape)\n",
    "print(\"w2:\", w2.shape)\n",
    "print(\"b2:\", b2.shape)\n",
    "batch_size = 500\n",
    "\n",
    "iterations = round(Ntr/batch_size)*300\n",
    "lr = 0.001\n",
    "lr_decay= 0.999\n",
    "reg = 5e-6\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "for t in range(iterations):\n",
    "    indices = np.random.choice(Ntr,batch_size)\n",
    "    rng.shuffle(indices)\n",
    "    # Forward pass\n",
    "    x = x_train[indices]\n",
    "    y = y_train[indices]\n",
    "    a = x.dot(w1)+b1\n",
    "    h = 1.0/(1+np.exp(-a))\n",
    "    y_pred = h.dot(w2)+b2\n",
    "\n",
    "    loss = 1./batch_size*np.square(y_pred-y).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # Backward pass\n",
    "    dy_pred = 1./batch_size*2.0*(y_pred-y)\n",
    "    dw2 = h.T.dot(dy_pred)+reg*w2\n",
    "    db2 = dy_pred.sum(axis=0)\n",
    "    dh = dy_pred.dot(w2.T)\n",
    "    dw1 = x.T.dot(dh*h*(1-h))+reg*w1\n",
    "    db1 = (dh*h*(1-h)).sum(axis=0)\n",
    "    w2-=lr*dw2\n",
    "    b2-=lr*db2\n",
    "    w1-=lr*dw1\n",
    "    b1-=lr*db1\n",
    "    lr*=lr_decay\n",
    "\n",
    "    # Printing accuracies and displaying w as images\n",
    "    if t%10==0:\n",
    "        print('iteration %d / %d : loss %f'%(t,iterations,loss))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}